{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MochamadPhilliaWibowo/Machine-Learning-Class/blob/main/Tugas%20Week%2013/BAB%202/Handling_multiple_sequences_(PyTorch).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nama : Mochamad Phillia Wibowo\n",
        "\n",
        "NIM : 1103204191\n",
        "\n",
        "Kelas : Machine Learning (TK-44-G04)\n",
        "\n",
        "Tugas 13 NLP HUGGING FACE (BAB 2)"
      ],
      "metadata": {
        "id": "D3FFOj67rYkq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLZH00QMpm7Q"
      },
      "source": [
        "# Handling multiple sequences (PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JEFuuavpm7a"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zBSIWlLNpm7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "022d50b7-23e4-48f7-8cb1-183eba9fc1f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.19.2-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.1/542.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate\n",
            "  Downloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Collecting requests>=2.32.1 (from datasets)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.4.3)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.1.99)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.20.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, requests, dill, multiprocess, datasets, evaluate\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.19.2 dill-0.3.8 evaluate-0.4.2 multiprocess-0.70.16 requests-2.32.3 xxhash-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code di atas adalah perintah untuk menginstal beberapa pustaka Python menggunakan pip, manajer paket standar untuk Python. Perintah ini melakukan instalasi tiga pustaka:\n",
        "\n",
        "datasets: Ini adalah pustaka yang menyediakan akses mudah ke berbagai kumpulan data untuk tugas-tugas pembelajaran mesin dan pemrosesan bahasa alami.\n",
        "\n",
        "evaluate: Ini mungkin adalah pustaka khusus yang digunakan untuk mengevaluasi model-model pembelajaran mesin atau pemrosesan bahasa alami, tetapi tanpa informasi lebih lanjut, sulit untuk mengkonfirmasi.\n",
        "\n",
        "transformers[sentencepiece]: Ini adalah pustaka yang populer digunakan untuk pemrosesan bahasa alami, terutama dalam bidang pemodelan bahasa dan transfer pembelajaran. Opsi [sentencepiece] menunjukkan bahwa instalasi juga akan mencakup dukungan untuk SentencePiece, sebuah pustaka tokenisasi teks multibahasa.\n",
        "\n",
        "Dengan menjalankan perintah ini, pustaka-pustaka ini akan diinstal dan siap digunakan dalam pengembangan aplikasi Python Anda."
      ],
      "metadata": {
        "id": "25WczwvAriOT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "N_qw7KH6pm7g",
        "outputId": "2273687e-60cc-400b-b51c-9752749e3145",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: POSITIVE\n",
            "Probabilities: tensor([[0.0037, 0.9963]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "input_ids = torch.tensor(ids).unsqueeze(0)  # Menambahkan dimensi batch\n",
        "\n",
        "# Sekarang Anda bisa menggunakan model\n",
        "outputs = model(input_ids)\n",
        "\n",
        "# Melakukan softmax terhadap logits untuk mendapatkan probabilitas kelas\n",
        "probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "\n",
        "# Membaca kelas yang diprediksi dari probabilitas\n",
        "id2label = model.config.id2label\n",
        "predicted_class_id = torch.argmax(probabilities, dim=-1).item()\n",
        "predicted_class_label = id2label[predicted_class_id]\n",
        "\n",
        "print(\"Predicted class:\", predicted_class_label)\n",
        "print(\"Probabilities:\", probabilities)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "import torch: Ini adalah baris yang mengimpor pustaka PyTorch, yang digunakan untuk operasi-operasi tensor dan pemrosesan di dalam model neural network.\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification: Baris ini mengimpor dua kelas dari pustaka Transformers:\n",
        "\n",
        "AutoTokenizer: Digunakan untuk menginisialisasi dan memuat tokenizer yang sesuai dengan model yang akan digunakan.\n",
        "AutoModelForSequenceClassification: Digunakan untuk menginisialisasi dan memuat model yang telah di-pretrained untuk tugas klasifikasi urutan teks.\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\": Ini adalah string yang berisi nama atau path model yang akan digunakan. Dalam hal ini, model yang dipilih adalah \"distilbert-base-uncased-finetuned-sst-2-english\", yang merupakan model DistilBERT yang telah di-fineturn pada tugas analisis sentimen (SST-2) dalam bahasa Inggris.\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint): Baris ini memuat tokenizer yang sesuai dengan model yang telah ditentukan dalam variabel checkpoint. Tokenizer ini akan digunakan untuk memproses teks sebelum diberikan kepada model.\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint): Baris ini memuat model yang sesuai dengan model yang telah ditentukan dalam variabel checkpoint. Model ini akan digunakan untuk melakukan klasifikasi urutan teks.\n",
        "\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\": Ini adalah teks yang akan diberikan kepada model untuk melakukan prediksi kelas.\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence): Baris ini menggunakan tokenizer yang telah diinisialisasi sebelumnya untuk memecah teks menjadi token-token. Tokenisasi memecah teks menjadi bagian-bagian yang lebih kecil yang disebut token, yang nantinya akan dimasukkan ke dalam model untuk analisis.\n",
        "\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens): Baris ini mengonversi token-token menjadi ID-token yang sesuai dengan kamus token yang digunakan oleh model. Ini diperlukan karena model menerima input dalam bentuk ID-token, bukan teks mentah.\n",
        "\n",
        "input_ids = torch.tensor(ids).unsqueeze(0): Baris ini mengubah ID-token menjadi tensor PyTorch dan menambahkan dimensi batch ke tensor tersebut. Model memerlukan input dalam bentuk batch, bahkan jika ukurannya hanya 1.\n",
        "\n",
        "outputs = model(input_ids): Baris ini menggunakan model untuk melakukan prediksi kelas berdasarkan input yang diberikan. Model akan menghasilkan output berupa tensor yang berisi nilai-nilai logits untuk setiap kelas.\n",
        "\n",
        "probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1): Baris ini menggunakan fungsi softmax untuk mengonversi logits yang dihasilkan oleh model menjadi probabilitas untuk setiap kelas. Ini memungkinkan kita untuk menafsirkan prediksi model sebagai probabilitas distribusi kelas.\n",
        "\n",
        "id2label = model.config.id2label: Baris ini mengakses atribut id2label dari konfigurasi model untuk mendapatkan pemetaan dari ID kelas ke label kelas. Hal ini membantu kita dalam menafsirkan output model sebagai label kelas.\n",
        "\n",
        "predicted_class_id = torch.argmax(probabilities, dim=-1).item(): Baris ini menggunakan argmax untuk mendapatkan indeks kelas yang memiliki probabilitas tertinggi, yang akan menjadi prediksi kelas dari model.\n",
        "\n",
        "predicted_class_label = id2label[predicted_class_id]: Baris ini mengonversi indeks kelas yang diprediksi menjadi label kelas menggunakan pemetaan yang telah diperoleh dari id2label.\n",
        "\n",
        "print(\"Predicted class:\", predicted_class_label): Baris ini mencetak label kelas yang diprediksi oleh model.\n",
        "\n",
        "print(\"Probabilities:\", probabilities): Baris ini mencetak probabilitas untuk setiap kelas yang dihasilkan oleh model. Probabilitas ini menunjukkan seberapa yakin model terhadap setiap kelas yang diprediksikan."
      ],
      "metadata": {
        "id": "MH8BVYB1rtLp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QRrv8g3Fpm7m",
        "outputId": "c55e2dd7-613c-4652-a850-268deb245a6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
            "          2607,  2026,  2878,  2166,  1012,   102]])\n"
          ]
        }
      ],
      "source": [
        "tokenized_inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
        "print(tokenized_inputs[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Baris ini menggunakan tokenizer yang telah diinisialisasi sebelumnya untuk memproses teks yang diberikan (sequence) dan menghasilkan representasi token dari teks tersebut dalam bentuk tensor PyTorch. Mari kita bahas lebih detail:\n",
        "\n",
        "tokenizer(sequence, return_tensors=\"pt\"): Ini adalah pemanggilan metode tokenizer yang menggunakan teks sequence sebagai input. Argumen return_tensors=\"pt\" menunjukkan bahwa kita ingin mengembalikan representasi token dalam bentuk tensor PyTorch. Ini akan menghasilkan keluaran yang merupakan dictionary yang berisi representasi token dari teks input dalam bentuk tensor PyTorch.\n",
        "\n",
        "tokenized_inputs[\"input_ids\"]: Ini adalah akses ke entri \"input_ids\" dari dictionary tokenized_inputs, yang berisi tensor PyTorch yang merepresentasikan token-token dari teks input dalam bentuk ID-token. Ini adalah representasi utama dari teks input yang akan dimasukkan ke dalam model untuk analisis lebih lanjut."
      ],
      "metadata": {
        "id": "RUwQ9gXLsYJ1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jsBw6ee6pm7n",
        "outputId": "cdfa6d56-aedd-4b6a-b8a6-ac8947337311",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs: tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
            "          2026,  2878,  2166,  1012]])\n",
            "Logits: tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "input_ids = torch.tensor([ids])\n",
        "print(\"Input IDs:\", input_ids)\n",
        "\n",
        "output = model(input_ids)\n",
        "print(\"Logits:\", output.logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ode ini melakukan hal-hal berikut:\n",
        "\n",
        "Menggunakan checkpoint yang telah ditentukan untuk memuat tokenizer dan model dari pustaka Transformers.\n",
        "\n",
        "Mengekstraksi token dari teks input menggunakan tokenizer dan mengonversi token-token tersebut menjadi ID-token.\n",
        "\n",
        "Menginisialisasi tensor PyTorch dari ID-token yang telah dihasilkan dan melakukan inferensi menggunakan model.\n",
        "\n",
        "Mencetak tensor PyTorch input_ids yang merepresentasikan ID-token dari teks input.\n",
        "\n",
        "Mencetak tensor logits yang dihasilkan oleh model, yang mewakili nilai-nilai mentah sebelum melewati fungsi aktivasi untuk memprediksi kelas dari teks input."
      ],
      "metadata": {
        "id": "wfyoQxcUsc-j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gtcuL25ppm7q"
      },
      "outputs": [],
      "source": [
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200]\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "h2i8fJ44pm7s"
      },
      "outputs": [],
      "source": [
        "padding_id = 100\n",
        "\n",
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, padding_id],\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "kRPCf7iHpm7u",
        "outputId": "eac826ed-a817-4c2e-84f4-ab20d29a41b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ 1.5694, -1.3895],\n",
            "        [ 1.3374, -1.2163]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence1_ids = [[200, 200, 200]]\n",
        "sequence2_ids = [[200, 200]]\n",
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, tokenizer.pad_token_id],\n",
        "]\n",
        "\n",
        "print(model(torch.tensor(sequence1_ids)).logits)\n",
        "print(model(torch.tensor(sequence2_ids)).logits)\n",
        "print(model(torch.tensor(batched_ids)).logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "UeIxvEa9pm7w",
        "outputId": "23710975-68e7-451a-f04d-f5ffa62dbd2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.5694, -1.3895],\n",
            "        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, tokenizer.pad_token_id],\n",
        "]\n",
        "\n",
        "attention_mask = [\n",
        "    [1, 1, 1],\n",
        "    [1, 1, 0],\n",
        "]\n",
        "\n",
        "outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\n",
        "print(outputs.logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "CuS37uMHpm7y"
      },
      "outputs": [],
      "source": [
        "max_sequence_length = 100  # Definisikan panjang maksimum yang diinginkan\n",
        "sequence = sequence[:max_sequence_length]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}